# -*- coding: utf-8 -*-
"""Customer Churn Supervised Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/138Rq9Bv3gMGyOGN8GmFYyr4d6C8D9BCm

## **Customer Churn Prediction**

The goal is to predict which telecom customers are likely to churn (leave the service) using their historical data. By building and evaluating machine learning models, the company can identify at-risk customers and take steps to improve retention

# **Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.tree import DecisionTreeClassifier

"""####Loading dataset from google drive"""

#mounting the drive
from google.colab import drive
drive.mount('/content/drive')

filepath_training = '/content/drive/MyDrive/Ict/Training_data.csv'

df_training_data = pd.read_csv(filepath_training)
df_training_data.head()

# Drop the column customer ID
df_training_data.drop('customerID', axis=1, inplace=True)

filepath_testing = '/content/drive/MyDrive/Ict/Testing_data.csv'

df_testing_data = pd.read_csv(filepath_testing)
df_testing_data.head()

# dropping the customer id column from test dataset
df_testing_data.drop('customerID', axis=1, inplace=True)

"""# **EDA**"""

# shape of training dataset
df_training_data.shape

#shape of test dataset
df_testing_data.shape

# training dataset
df_training_data.describe()

# testing dataset
df_testing_data.describe()

# info of training dataset
df_training_data.info()

#info of testing dataset
df_testing_data.info()

"""**Duplicates**"""

# Checking Duplicates in Training Dataset
df_training_data.duplicated().sum()

# removing duplicate rows
df_training_data.drop_duplicates(inplace=True)

# Checking Duplicates in Testing Dataset
df_testing_data.duplicated().sum()

# removing duplicate rows
df_testing_data.drop_duplicates(inplace=True)

"""**Null Values**"""

#missing values in training dataset
df_training_data.isna().sum()

#missing values in testing dataset
df_testing_data.isna().sum()

# number of unique values in each column of training dataset
df_training_data.nunique()

# number of unique values in each column of testing dataset
df_testing_data.nunique()

"""**#  Identifying Outliers**

Training Dataset
"""

numeric_cols = df_training_data.select_dtypes(include=['number']) # Select only numerical columns
numeric_cols.columns

#finding outliers
outlier_counts = {} #empty dict for storing outliers if any

for col in numeric_cols.columns:
    Q1 = df_training_data[col].quantile(0.25)
    Q3 = df_training_data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    count = ((df_training_data[col] < lower) | (df_training_data[col] > upper)).sum() #sum of the outliers present
    outlier_counts[col] = count

print(outlier_counts['SeniorCitizen'])
print(outlier_counts['tenure'])
print(outlier_counts['MonthlyCharges'])

# correlation
#correlation matrix(numerical columns only because data is not preprocessed)
corr_matrix = numeric_cols.corr()
corr_matrix

#Heat map with the corelation matrix
plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

#distribution plot of numeric columns
for col in numeric_cols:
    plt.figure(figsize=(6, 3))
    sns.histplot(df_training_data[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()

"""Testing dataset"""

numeric_cols = df_testing_data.select_dtypes(include=['number']) # Select only numerical columns
numeric_cols.columns

#finding outliers
outlier_counts = {} #empty dict for storing outliers if any

for col in numeric_cols.columns:
    Q1 = df_testing_data[col].quantile(0.25)
    Q3 = df_testing_data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    count = ((df_testing_data[col] < lower) | (df_testing_data[col] > upper)).sum() #sum of the outliers present
    outlier_counts[col] = count

print(outlier_counts['SeniorCitizen'])
print(outlier_counts['tenure'])
print(outlier_counts['MonthlyCharges'])

# correlation
#correlation matrix(numerical columns only because data is not preprocessed)
corr_matrix = numeric_cols.corr()
corr_matrix

#Heat map with the corelation matrix
plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

#distribution plot of numeric columns
for col in numeric_cols:
    plt.figure(figsize=(6, 3))
    sns.histplot(df_training_data[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()

"""# **Preprocessing**"""

df_training_data.info()

# count of missing values in each column of training dataset
df_training_data.isna().sum()

# Filling missing values in Gender using mode
df_training_data['gender'] = df_training_data['gender'].fillna(df_training_data['gender'].mode()[0])

# Filling missing values in OnlineSecurity using mode
df_training_data['OnlineSecurity'] = df_training_data['OnlineSecurity'].fillna(df_training_data['OnlineSecurity'].mode()[0])

# Skew Score
df_training_data['MonthlyCharges'].skew()  # Less than 0 means negative skew

mean_val = df_training_data['MonthlyCharges'].mean()
median_val = df_training_data['MonthlyCharges'].median()
print(mean_val)
print(median_val)      # Mean < Median = Negative skew(left skewed)

# Filling missing values in MonthlyCharges using median as median is less sensitive to outliers
df_training_data['MonthlyCharges'] = df_training_data['MonthlyCharges'].fillna(df_training_data['MonthlyCharges'].median())

# Converting TotalCharges to numeric
df_training_data['TotalCharges'] = pd.to_numeric(df_training_data['TotalCharges'], errors='coerce')

df_training_data['TotalCharges'].skew() # Greater than 0 means positive skew

mean_val = df_training_data['TotalCharges'].mean()
median_val = df_training_data['TotalCharges'].median()
print(mean_val)
print(median_val)      # Mean > Median = Positive skew(right skewed)

# Filling missing values in TotalCharges using median as median is less sensitive to outliers
df_training_data['TotalCharges'] = df_training_data['TotalCharges'].fillna(df_training_data['TotalCharges'].median())

df_training_data.isna().sum()

df_testing_data.isna().sum()

# Converting TotalCharges of test data to numeric
df_testing_data['TotalCharges'] = pd.to_numeric(df_testing_data['TotalCharges'], errors='coerce')

"""Outlier Analysis"""

# Training data
numeric_cols = df_training_data.select_dtypes(include=['int64', 'float64']).columns
for col in numeric_cols:
    plt.figure(figsize=(6, 4))
    sns.boxplot(df_training_data[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

# Clipping outliers of TotalCharges
Q1 = df_training_data['TotalCharges'].quantile(0.25)
Q3 = df_training_data['TotalCharges'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df_training_data['TotalCharges'] = df_training_data['TotalCharges'].clip(lower_bound, upper_bound)

sns.boxplot(df_training_data['TotalCharges'])

# Test data
numeric_cols = df_testing_data.select_dtypes(include=['int64', 'float64']).columns
for col in numeric_cols:
    plt.figure(figsize=(6, 4))
    sns.boxplot(df_testing_data[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

"""# **Encoding Categorical Values**

Training data
"""

df_training_data.info()

# LabelEncoding on Churn column
label_enc = LabelEncoder()
df_training_data['Churn'] = LabelEncoder().fit_transform(df_training_data['Churn'])
df_training_data.head()

df_training_data['gender'] = df_training_data['gender'].map({'Female': 1, 'Male': 0})
df_training_data['Partner'] = df_training_data['Partner'].map({'Yes': 1, 'No': 0})
df_training_data['Dependents'] = df_training_data['Dependents'].map({'Yes': 1, 'No': 0})
df_training_data['PhoneService'] = df_training_data['PhoneService'].map({'Yes': 1, 'No': 0})
df_training_data['PaperlessBilling'] = df_training_data['PaperlessBilling'].map({'Yes': 1, 'No': 0})

binary_service_cols = [
    'MultipleLines', 'OnlineSecurity', 'OnlineBackup',
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'
]

for col in binary_service_cols:
    df_training_data[col] = df_training_data[col].replace({'No internet service': 'No', 'No phone service': 'No'})
    df_training_data[col] = df_training_data[col].map({'Yes': 1, 'No': 0})

df_training_data['Contract'].unique()

df_training_data['Contract'] = df_training_data['Contract'].map({'Month-to-month': 1, 'One year': 12, 'Two year': 24})

df_training_data = pd.get_dummies(df_training_data, columns=['InternetService', 'PaymentMethod'], dtype = int)

df_training_data.info()

"""####Feature Engineering"""

# Create interaction feature
df_training_data['Tenure_Contract'] = df_training_data['tenure'] * df_training_data['Contract']

df_training_data['CLV'] = df_training_data['tenure'] * df_training_data['MonthlyCharges']

"""####Scaling on training data"""

# pairplot to identify type of scaling to be used
sns.pairplot(df_training_data[['SeniorCitizen','tenure','MonthlyCharges','TotalCharges','Tenure_Contract','CLV']])

min_max_scaler = MinMaxScaler(feature_range = (0,1))
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges','Tenure_Contract','CLV']
df_training_data[num_cols]=min_max_scaler.fit_transform(df_training_data[num_cols])
df_training_data

"""#####Test data"""

df_testing_data.info()

# LabelEncoding on Churn column
label_enc = LabelEncoder()
df_testing_data['Churn'] = LabelEncoder().fit_transform(df_testing_data['Churn'])
df_testing_data.head()

df_testing_data['gender'] = df_testing_data['gender'].map({'Female': 1, 'Male': 0})
df_testing_data['Partner'] = df_testing_data['Partner'].map({'Yes': 1, 'No': 0})
df_testing_data['Dependents'] = df_testing_data['Dependents'].map({'Yes': 1, 'No': 0})
df_testing_data['PhoneService'] = df_testing_data['PhoneService'].map({'Yes': 1, 'No': 0})
df_testing_data['PaperlessBilling'] = df_testing_data['PaperlessBilling'].map({'Yes': 1, 'No': 0})

binary_service_cols = [
    'MultipleLines', 'OnlineSecurity', 'OnlineBackup',
    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'
]

for col in binary_service_cols:
    df_testing_data[col] = df_testing_data[col].replace({'No internet service': 'No', 'No phone service': 'No'})
    df_testing_data[col] = df_testing_data[col].map({'Yes': 1, 'No': 0})

df_testing_data['Contract'] = df_testing_data['Contract'].map({'Month-to-month': 1, 'One year': 12, 'Two year': 24})

df_testing_data = pd.get_dummies(df_testing_data, columns=['InternetService', 'PaymentMethod'], dtype = int)

df_testing_data.info()

# Create interaction feature
df_testing_data['Tenure_Contract'] = df_testing_data['tenure'] * df_testing_data['Contract']

df_testing_data['CLV'] = df_testing_data['tenure'] * df_testing_data['MonthlyCharges']

"""Scaling on Test data"""



# pairplot to identify type of scaling to be used
sns.pairplot(df_testing_data[['SeniorCitizen','tenure','MonthlyCharges','TotalCharges','Tenure_Contract','CLV']])

min_max_scaler = MinMaxScaler(feature_range = (0,1))
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges','Tenure_Contract','CLV']
df_testing_data[num_cols]=min_max_scaler.fit_transform(df_testing_data[num_cols])
df_testing_data

"""####Model Building"""

X_train = df_training_data.drop('Churn', axis=1)
y_train = df_training_data['Churn']

X_test = df_testing_data.drop('Churn', axis=1)
y_test = df_testing_data['Churn']

"""####Model Implementation


"""

# Initialize models
log_model = LogisticRegression(max_iter=1000)
rf_model = RandomForestClassifier()
svm_model = SVC(probability=True)

# Fit models
log_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)
svm_model.fit(X_train, y_train)

"""####Model Evaluation"""

def evaluate_model(model, name):
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    print(f"\n Evaluation for {name}")
    print(f"Accuracy  : {accuracy_score(y_test, y_pred):.4f}")
    print(f"Precision : {precision_score(y_test, y_pred):.4f}")
    print(f"Recall    : {recall_score(y_test, y_pred):.4f}")
    print(f"F1 Score  : {f1_score(y_test, y_pred):.4f}")


# Filling missing values in TotalCharges of test data using median
df_testing_data['TotalCharges'] = df_testing_data['TotalCharges'].fillna(df_testing_data['TotalCharges'].median())

# Re-assign X_test after filling NaN values
X_test = df_testing_data.drop('Churn', axis=1)
y_test = df_testing_data['Churn']

# Evaluate each model with the corrected X_test
evaluate_model(log_model, "Logistic Regression")
evaluate_model(rf_model, "Random Forest")
evaluate_model(svm_model, "Support Vector Machine")



results = []

for model, name in zip([log_model, rf_model, svm_model],
                       ["Logistic Regression", "Random Forest", "SVM"]):
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    results.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1 Score': f1_score(y_test, y_pred)
    })

results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by='F1 Score', ascending=False, ignore_index=True)

"""####Logistics Regression is better in accuracy than SVM and random forest."""

results_df

"""Model Fine-Tuning

RandomizedSearchCV
"""

# Define parameter grid
param_dist = {
    'n_estimators': np.arange(100, 501, 50),
    'max_depth': [None, 5, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 15],
    'min_samples_leaf': [1, 2, 4, 6],
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True, False]}

# Initialize model
rf = RandomForestClassifier(random_state=42)

# Randomized Search CV
rf_random = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=50,               # number of combinations to try
    cv=5,                    # 5-fold cross validation
    scoring='roc_auc',       # optimize for ROC-AUC
    random_state=42,
    n_jobs=-1,               # use all CPU cores
    verbose=2
)

# Fit on training data
rf_random.fit(X_train, y_train)

from sklearn.metrics import classification_report, roc_auc_score

# Best parameters
print("\nBest Parameters:", rf_random.best_params_)

# Evaluate tuned model
best_rf = rf_random.best_estimator_
y_pred = best_rf.predict(X_test)
y_proba = best_rf.predict_proba(X_test)[:, 1]

print("\nClassification Report (Tuned RF):")
print(classification_report(y_test, y_pred))
print("ROC-AUC (Tuned RF):", roc_auc_score(y_test, y_proba))